---
title: Configure LLM Provider
slug: /guides/configure-llm
description: 配置内置或自定义 LLM 提供商，让 Aelin 在稳定性、时延与回答质量之间取得更好的平衡。
---

# Configure LLM Provider

这篇指南帮助你完成一件很实际的事：让 Aelin 的模型链路稳定可用。

配置模型并不难，但如果忽略了几个关键项，后续会出现“看起来能用、实际不稳定”的情况。下面按最稳妥的流程来。

## 基本配置项

你通常需要填写以下信息：

- Provider 名称（可自定义，便于区分）
- Base URL
- Model
- API Key
- Temperature（建议先保守，再逐步调整）

如果你同时接入多个 provider，建议给名称加上用途标记，例如：`general-chat`、`analysis-fast`、`long-context`。

## 推荐配置流程

1. 先填入基础参数，不做过度微调。
2. 执行连通性测试，确认请求链路可达。
3. 用 2 到 3 条真实问题做小样本验证。
4. 再根据效果调节温度与模型选择。

这个顺序的好处是：你能快速定位问题来自“网络/认证”还是“模型行为”。

## 质量与成本平衡建议

- 先选择稳定模型，后考虑高性能模型。
- 如果响应慢，先排查网络与模型负载，再调参数。
- 不要一开始把温度设得太高，避免回答风格波动。

对多数场景来说，“稳定 + 可解释”通常比“偶尔惊艳但不可控”更有长期价值。

## 常见问题

- 只返回模板话术：通常是 API Key、Base URL 或模型名不匹配。
- 响应明显变慢：可能是模型负载高、上下文过长或网络波动。
- 移动端失败：确认 `VITE_MOBILE_API_BASE_URL` 是否可被设备访问。

## 进阶建议

当你准备上线到长期使用阶段时，建议保留一份“已验证配置快照”：

- 记录可用 provider、model、参数与测试样例。
- 在重大升级前做一次回归验证。

这样即使后续切换模型，也能快速回到稳定基线。
